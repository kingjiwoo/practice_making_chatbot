{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs+1Rfp9Ldr05CH2EbrklP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingjiwoo/practice_making_chatbot/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvGlr8OlFx7t"
      },
      "outputs": [],
      "source": [
        "#import module\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocess import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#에폭당 정확도와 손실값을 시각화하는 함수\n",
        "def plot_graph(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "#경로 정리 \n",
        "DATA_IN_PATH = '/content/drive/MyDrive/data_in/'\n",
        "DATA_OUT_PATH = '/content/drive/MyDrive/data_in/'\n",
        "TRAIN_INPUTS = 'train_input.npy'\n",
        "TRAIN_OUTPUTS = 'train_output.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "#랜덤 시드값 선언 \n",
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)\n"
      ],
      "metadata": {
        "id": "9nCzKNfYF7RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'rb'))\n",
        "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS, 'rb'))\n",
        "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))\n",
        "\n",
        "print(len(index_inputs), len(index_outputs), len(index_targets))"
      ],
      "metadata": {
        "id": "85arH8uUF9MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델에 필요한 값 선언 \n",
        "MODEL_NAME = 'seq2seq_kor'\n",
        "BATCH_SIZE = 2\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCH = 50\n",
        "UNITS = 1024\n",
        "EMBEDDING_DIM = 256\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']\n",
        "std_index = prepro_configs['std_symbol']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "vocab_size = prepro_configs['vocab_size']\n"
      ],
      "metadata": {
        "id": "F_u3xGiBGAgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_batch_ops import batch\n",
        "#2014년 조경션 교수님이 사용한 GRU 모델 사용\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size \n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences = True,\n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer = 'glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self, inp):\n",
        "        return tf.zeros((tf.shape(inp)[0], self.enc_units))"
      ],
      "metadata": {
        "id": "Ohrjha9yGC2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#어텐션\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "\n",
        "        context_vector = attention_weights * values \n",
        "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "8nS1eQCSGEaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#디코더 블록\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim \n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences = True,\n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer = 'glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis =-1)\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x , state, attention_weights"
      ],
      "metadata": {
        "id": "kDVdBMtBGIzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#손실함수와 정확도 측정 함수 \n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "#손실함수를 측정하기 위한 함수 \n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "#정확도를 측정하기 위한 함수 \n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask\n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "metadata": {
        "id": "PWFCHgELGLGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_sz,end_token_idx=2):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_sz)\n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_sz)\n",
        "\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output,enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        predict_tokens = list()\n",
        "        for t in range(0, tar.shape[1]):\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:,t], 1), tf.float32)\n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
        "\n",
        "        return tf.stack(predict_tokens, axis= 1)\n",
        "\n",
        "    def inference(self, x):\n",
        "        inp = x \n",
        "\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([char2idx[std_index]], 1)\n",
        "\n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_token = tf.argmax(predictions[0])\n",
        "\n",
        "            if predict_token == self.end_token_idx:\n",
        "                break\n",
        "\n",
        "            predict_tokens.append(predict_token)\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)\n",
        "\n",
        "        return tf.stack(predict_tokens, axis = 0).numpy()"
      ],
      "metadata": {
        "id": "tsKbb--jGOKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq2seq\n",
        "\n",
        "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, char2idx[end_index])\n",
        "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])"
      ],
      "metadata": {
        "id": "MbD793QHGRhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = DATA_OUT_PATH + MODEL_NAME\n",
        "if not(os.path.isdir(PATH)):\n",
        "    os.makedirs(os.path.join(PATH))\n",
        "\n",
        "checkpoint_path = DATA_OUT_PATH + MODEL_NAME + '/weights.h5'\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor = 'val_accuracy', verbose=1, save_best_only=True,\n",
        "    save_weights_only = True\n",
        ")\n",
        "\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "\n",
        "history = model.fit([index_inputs, index_outputs], index_targets,\n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCH,\n",
        "                    validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "metadata": {
        "id": "qY9t2jlvGTuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}